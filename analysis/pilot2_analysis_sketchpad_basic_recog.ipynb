{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'analysis_helpers' from 'analysis_helpers.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib, cStringIO\n",
    "\n",
    "import pymongo as pm\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "\n",
    "import analysis_helpers as h\n",
    "reload(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory & file hierarchy\n",
    "iterationName = 'pilot2'\n",
    "exp_path = './'\n",
    "analysis_dir = os.getcwd()\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(),'../../..','data',exp_path))\n",
    "exp_dir = './'\n",
    "sketch_dir = os.path.abspath(os.path.join(os.getcwd(),'../../..','analysis',exp_path,'sketches','pilot2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv('auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'rxdhawkins.me' ## cocolab ip address\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "import pymongo as pm\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')\n",
    "db = conn['3dObjects']\n",
    "coll = db['sketchpad_basic_recog']\n",
    "\n",
    "stimdb = conn['stimuli']\n",
    "stimcoll = stimdb['sketchpad_basic_pilot2_sketches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553 sketches in the stimuli db that have been retrieved at least once\n",
      "551 records in the recognition experiment database\n"
     ]
    }
   ],
   "source": [
    "## How many sketches have been retrieved at least once? equivalent to: coll.find({'numGames':{'$exists':1}}).count()\n",
    "x = stimcoll.find({'numGames':{'$gte':0}}).count()\n",
    "y = coll.count()\n",
    "print '{} sketches in the stimuli db that have been retrieved at least once'.format(x)\n",
    "print '{} records in the recognition experiment database'.format(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess recognition task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 550 records analyzed.\n",
      "50 out of 550 records analyzed.\n",
      "100 out of 550 records analyzed.\n",
      "150 out of 550 records analyzed.\n",
      "200 out of 550 records analyzed.\n",
      "250 out of 550 records analyzed.\n",
      "300 out of 550 records analyzed.\n",
      "350 out of 550 records analyzed.\n",
      "400 out of 550 records analyzed.\n",
      "450 out of 550 records analyzed.\n"
     ]
    }
   ],
   "source": [
    "## retrieve records from db\n",
    "## notes: \n",
    "## pilot0 = no feedback onscreen\n",
    "## pilot1 = bonus point counter onscreen\n",
    "\n",
    "a = coll.find({'iterationName':'pilot1'}).sort('gameID')\n",
    "\n",
    "## make lists from db\n",
    "gameID = []\n",
    "target = []\n",
    "choice = []\n",
    "correct = []\n",
    "correct_class = []\n",
    "rt = []\n",
    "fname = []\n",
    "\n",
    "d1 = []\n",
    "d2 = []\n",
    "d3 = []\n",
    "target_category = []\n",
    "chosen_category = []\n",
    "condition = []\n",
    "drawDuration = []\n",
    "original_gameID = []\n",
    "viewer_correct = []\n",
    "viewer_choice = []\n",
    "viewer_RT = []\n",
    "mean_intensity = []\n",
    "num_strokes = []\n",
    "\n",
    "bad_sessions = ['1571-00d11ddf-96e7-4aae-ba09-1a338b328c0e']\n",
    "\n",
    "counter = 0\n",
    "for rec in a:\n",
    "    if rec['gameID'] not in bad_sessions: \n",
    "        if counter%50==0:\n",
    "            print '{} out of {} records analyzed.'.format(counter,a.count())\n",
    "        gameID.append(rec['gameID'])\n",
    "        target.append(rec['target'])\n",
    "        choice.append(rec['choice'])\n",
    "        correct.append(rec['correct'])\n",
    "        rt.append(rec['rt'])\n",
    "        f = rec['sketch'].split('/')[-1]\n",
    "        fname.append(f)\n",
    "        chosen_category.append(h.objcat[rec['choice']])\n",
    "\n",
    "        ## match up with corresponding record in stimuli collection\n",
    "        b = stimcoll.find({'fname_no_target':f})[0]\n",
    "        assert stimcoll.find({'fname_no_target':f}).count()==1\n",
    "        d1.append(b['Distractor1'])\n",
    "        d2.append(b['Distractor2'])\n",
    "        d3.append(b['Distractor2'])\n",
    "        target_category.append(b['category'])\n",
    "        correct_class.append(h.objcat[rec['choice']]==b['category'])\n",
    "        condition.append(b['condition'])\n",
    "        drawDuration.append(b['drawDuration'])\n",
    "        original_gameID.append(b['gameID'])\n",
    "        viewer_correct.append(b['outcome'])\n",
    "        viewer_choice.append(b['response'])\n",
    "        viewer_RT.append(b['viewerRT'])\n",
    "        mean_intensity.append(b['mean_intensity'])  \n",
    "        num_strokes.append(b['numStrokes'])    \n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486 annotations saved.\n"
     ]
    }
   ],
   "source": [
    "## organize data into dataframe\n",
    "X = pd.DataFrame([gameID,target,choice,correct,rt,fname,d1,d2,d3,target_category,chosen_category,condition,drawDuration, \\\n",
    "                 original_gameID,viewer_correct,viewer_choice,viewer_RT,mean_intensity,num_strokes,correct_class])\n",
    "X = X.transpose()\n",
    "X.columns = ['gameID','target','choice','correct','rt','fname','d1','d2','d3','target_category','chosen_category','condition','drawDuration', \\\n",
    "            'original_gameID','viewer_correct','viewer_choice','viewer_RT','mean_intensity','num_strokes','correct_class']\n",
    "print '{} annotations saved.'.format(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic performance measures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./plots'):\n",
    "    os.makedirs('./plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition\n",
      "closer     0.590308\n",
      "further    0.451737\n",
      "Name: correct, dtype: float64\n",
      "condition\n",
      "closer     0.986784\n",
      "further    0.992278\n",
      "Name: correct_class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## what is object-level accuracy broken out condition?\n",
    "print X.groupby('condition')['correct'].apply(lambda x: np.mean(x))\n",
    "\n",
    "## what is class-level accuracy?\n",
    "print X.groupby('condition')['correct_class'].apply(lambda x: np.mean(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition\n",
      "closer     6261.0\n",
      "further    6591.0\n",
      "Name: rt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print X.groupby('condition')['rt'].apply(lambda x: np.median(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## plot accuracy by condition and game (individual differences)\n",
    "#X.groupby(['gameID','condition'])['correct'].apply(lambda x: np.mean(x))\n",
    "game_acc_close = X[X['condition']=='closer'].groupby('gameID')['correct'].apply(lambda x: np.mean(x))\n",
    "game_acc_far = X[X['condition']=='further'].groupby('gameID')['correct'].apply(lambda x: np.mean(x))\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.scatter(game_acc_close,game_acc_far)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.plot([0,1],[0,1],linestyle='dashed')\n",
    "plt.title('accuracy by condition and game')\n",
    "plt.xlabel('close accuracy')\n",
    "plt.ylabel('far accuracy')\n",
    "plt.savefig('./plots/accuracy_by_condition_and_game.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## overall accuracy by category\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "sns.barplot(x='target_category',y='correct',data=X)\n",
    "plt.xlabel('category')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy by category')\n",
    "plt.ylim([0,1])\n",
    "plt.savefig('./plots/accuracy_by_category.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objs = np.unique(X['target'].values)\n",
    "obj_acc_close = X[X['condition']=='closer'].groupby('target')['correct'].apply(lambda x: np.mean(x))\n",
    "obj_acc_far = X[X['condition']=='further'].groupby('target')['correct'].apply(lambda x: np.mean(x))\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.scatter(obj_acc_close,obj_acc_far)\n",
    "for i, txt in enumerate(objs):\n",
    "    plt.annotate(txt, (obj_acc_close[i],obj_acc_far[i]))\n",
    "plt.xlim([-0.1,1])\n",
    "plt.ylim([-0.1,1]) \n",
    "plt.plot([0,1],[0,1],linestyle='dashed')\n",
    "plt.xlabel('close accuracy')\n",
    "plt.ylabel('far accuracy')\n",
    "plt.title('accuracy by condition and object')\n",
    "plt.savefig('./plots/accuracy_by_condition_and_object.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objs = np.unique(X['target'].values)\n",
    "obj_acc_close = X[X['condition']=='closer'].groupby('target')['rt'].apply(lambda x: np.mean(x))\n",
    "obj_acc_far = X[X['condition']=='further'].groupby('target')['rt'].apply(lambda x: np.mean(x))\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.scatter(obj_acc_close,obj_acc_far)\n",
    "for i, txt in enumerate(objs):\n",
    "    plt.annotate(txt, (obj_acc_close[i],obj_acc_far[i]))\n",
    "plt.xlim([0,20000])\n",
    "plt.ylim([0,20000])\n",
    "plt.plot([0,20000],[0,20000],linestyle='dashed')\n",
    "plt.xlabel('close RT')\n",
    "plt.ylabel('far RT')\n",
    "plt.title('RT by condition and object')\n",
    "plt.savefig('./plots/RT_by_condition_and_object.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Y = X.groupby(['target','condition'])['correct'].apply(lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import analysis_helpers as h\n",
    "reload(h)\n",
    "\n",
    "## get standardized object list\n",
    "categories = ['bird','car','chair','dog']\n",
    "obj_list = []\n",
    "for cat in categories:\n",
    "    for i,j in h.objcat.iteritems():\n",
    "        if j==cat:\n",
    "            obj_list.append(i)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### all sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## initialize confusion matrix\n",
    "confusion = np.zeros((len(obj_list),len(obj_list)))\n",
    "\n",
    "## generate confusion matrix by incrementing in each cell\n",
    "for i,d in X.iterrows():\n",
    "    targ_ind = obj_list.index(d['target'])\n",
    "    choice_ind = obj_list.index(d['choice'])\n",
    "    confusion[targ_ind,choice_ind] += 1\n",
    "    \n",
    "## normalized confusion matrix    \n",
    "normed = np.zeros((len(obj_list),len(obj_list)))\n",
    "for i in np.arange(len(confusion)):\n",
    "    normed[i,:] = confusion[i,:]/np.sum(confusion[i,:])    \n",
    "    \n",
    "## plot confusion matrix\n",
    "from matplotlib import cm\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(111)\n",
    "cax = ax.matshow(normed,vmin=0,vmax=1,cmap=cm.viridis)\n",
    "plt.xticks(range(len(normed)), obj_list, fontsize=12,rotation='vertical')\n",
    "plt.yticks(range(len(normed)), obj_list, fontsize=12)\n",
    "plt.colorbar(cax,shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./plots/confusion_matrix_all.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### divided by condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conds = ['closer','further']\n",
    "\n",
    "for cond in conds:\n",
    "    ## initialize confusion matrix \n",
    "    confusion = np.zeros((len(obj_list),len(obj_list)))\n",
    "\n",
    "    _X = X[X['condition']==cond]\n",
    "    ## generate confusion matrix by incrementing in each cell\n",
    "    for i,d in _X.iterrows():\n",
    "        targ_ind = obj_list.index(d['target'])\n",
    "        choice_ind = obj_list.index(d['choice'])\n",
    "        confusion[targ_ind,choice_ind] += 1\n",
    "\n",
    "    ## normalized confusion matrix    \n",
    "    normed = np.zeros((len(obj_list),len(obj_list)))\n",
    "    for i in np.arange(len(confusion)):\n",
    "        normed[i,:] = confusion[i,:]/np.sum(confusion[i,:])    \n",
    "\n",
    "    ## plot confusion matrix\n",
    "    from matplotlib import cm\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = plt.subplot(111)\n",
    "    cax = ax.matshow(normed,vmin=0,vmax=1,cmap=cm.viridis)\n",
    "    plt.xticks(range(len(normed)), obj_list, fontsize=12,rotation='vertical')\n",
    "    plt.yticks(range(len(normed)), obj_list, fontsize=12)\n",
    "    plt.colorbar(cax,shrink=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./plots/confusion_matrix_{}.pdf'.format(cond))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
