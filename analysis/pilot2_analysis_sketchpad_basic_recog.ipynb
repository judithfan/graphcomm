{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib, cStringIO\n",
    "\n",
    "import pymongo as pm\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "sns.set_style('white')\n",
    "\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "import base64\n",
    "import json\n",
    "\n",
    "import analysis_helpers as h\n",
    "reload(h)\n",
    "\n",
    "## get standardized object list\n",
    "categories = ['bird','car','chair','dog']\n",
    "obj_list = []\n",
    "for cat in categories:\n",
    "    for i,j in h.objcat.iteritems():\n",
    "        if j==cat:\n",
    "            obj_list.append(i)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory & file hierarchy\n",
    "iterationName = 'pilot2'\n",
    "exp_path = './'\n",
    "analysis_dir = os.getcwd()\n",
    "data_dir = os.path.abspath(os.path.join(os.getcwd(),'../../..','data',exp_path))\n",
    "exp_dir = './'\n",
    "sketch_dir = os.path.abspath(os.path.join(os.getcwd(),'../../..','analysis',exp_path,'sketches','pilot2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv('auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'rxdhawkins.me' ## cocolab ip address\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "import pymongo as pm\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1')\n",
    "db = conn['3dObjects']\n",
    "coll = db['sketchpad_basic_recog']\n",
    "\n",
    "stimdb = conn['stimuli']\n",
    "stimcoll = stimdb['sketchpad_basic_pilot2_sketches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## How many sketches have been retrieved at least once? equivalent to: coll.find({'numGames':{'$exists':1}}).count()\n",
    "x = stimcoll.find({'numGames':{'$gte':0}}).count()\n",
    "y = coll.count()\n",
    "print '{} sketches in the stimuli db that have been retrieved at least once'.format(x)\n",
    "print '{} records in the recognition experiment database'.format(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(coll.find({'iterationName': {'$in': ['pilot0', 'pilot1']}}).distinct('wID'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess recognition task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## retrieve records from db\n",
    "## notes: \n",
    "## pilot0 = no feedback onscreen\n",
    "## pilot1 = bonus point counter onscreen\n",
    "\n",
    "a = coll.find({'iterationName': {'$in': ['pilot0', 'pilot1']}}).sort('gameID')\n",
    "\n",
    "## make lists from db\n",
    "gameID = []\n",
    "target = []\n",
    "choice = []\n",
    "correct = []\n",
    "correct_class = []\n",
    "rt = []\n",
    "fname = []\n",
    "\n",
    "d1 = []\n",
    "d2 = []\n",
    "d3 = []\n",
    "target_category = []\n",
    "chosen_category = []\n",
    "condition = []\n",
    "drawDuration = []\n",
    "original_gameID = []\n",
    "viewer_correct = []\n",
    "viewer_choice = []\n",
    "viewer_RT = []\n",
    "mean_intensity = []\n",
    "num_strokes = []\n",
    "\n",
    "bad_sessions = ['1571-00d11ddf-96e7-4aae-ba09-1a338b328c0e','9770-2f360e9a-7a07-4026-9c36-18b558c1da21']\n",
    "\n",
    "counter = 0\n",
    "for rec in a:\n",
    "    if rec['gameID'] not in bad_sessions:\n",
    "        try:\n",
    "            if counter%500==0:\n",
    "                print '{} out of {} records analyzed.'.format(counter,a.count())\n",
    "            if rec['target'] is not None:\n",
    "                gameID.append(rec['gameID'])\n",
    "                target.append(rec['target'])\n",
    "                choice.append(rec['choice'])\n",
    "                correct.append(rec['correct'])\n",
    "                rt.append(rec['rt'])\n",
    "                f = rec['sketch'].split('/')[-1]\n",
    "                fname.append(f)\n",
    "                chosen_category.append(h.objcat[rec['choice']])\n",
    "\n",
    "                ## match up with corresponding record in stimuli collection\n",
    "                b = stimcoll.find({'fname_no_target':f})[0]\n",
    "                assert stimcoll.find({'fname_no_target':f}).count()==1\n",
    "                d1.append(b['Distractor1'])\n",
    "                d2.append(b['Distractor2'])\n",
    "                d3.append(b['Distractor2'])\n",
    "                target_category.append(b['category'])\n",
    "                correct_class.append(h.objcat[rec['choice']]==b['category'])\n",
    "                condition.append(b['condition'])\n",
    "                drawDuration.append(b['drawDuration'])\n",
    "                original_gameID.append(b['gameID'])\n",
    "                viewer_correct.append(b['outcome'])\n",
    "                viewer_choice.append(b['response'])\n",
    "                viewer_RT.append(b['viewerRT'])\n",
    "                mean_intensity.append(b['mean_intensity'])  \n",
    "                num_strokes.append(b['numStrokes'])    \n",
    "                counter += 1\n",
    "        except:\n",
    "            print 'Something went wrong with {} {}'.format(rec['gameID'],rec['trialNum'])\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## organize data into dataframe\n",
    "X = pd.DataFrame([gameID,target,choice,correct,rt,fname,d1,d2,d3,target_category,chosen_category,condition,drawDuration, \\\n",
    "                 original_gameID,viewer_correct,viewer_choice,viewer_RT,mean_intensity,num_strokes,correct_class])\n",
    "X = X.transpose()\n",
    "X.columns = ['gameID','target','choice','correct','rt','fname','d1','d2','d3','target_category','chosen_category','condition','drawDuration', \\\n",
    "            'original_gameID','viewer_correct','viewer_choice','viewer_RT','mean_intensity','num_strokes','correct_class']\n",
    "print '{} annotations saved.'.format(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## remove NaN rows from data matrix (no target recorded)\n",
    "X = X[X['target'].isin(obj_list)]\n",
    "\n",
    "## filter out responses that took too long, or too short\n",
    "too_fast = 1000\n",
    "too_slow = 20000\n",
    "X = X[(X['rt']>=too_fast) & (X['rt']<=too_slow)]\n",
    "\n",
    "print '{} annotations retained.'.format(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## save out to CSV\n",
    "X.to_csv('./sketchpad_basic_recog_group_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic performance measures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load in CSV\n",
    "X = pd.read_csv('./sketchpad_basic_recog_group_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./plots'):\n",
    "    os.makedirs('./plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## how many sessions (proxy for number of observers) do we have?\n",
    "print 'Number of sessions: {}'.format(len(np.unique(X.gameID.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## what is object-level accuracy broken out condition?\n",
    "print X.groupby('condition')['correct'].apply(lambda x: np.mean(x))\n",
    "\n",
    "## what is class-level accuracy?\n",
    "print X.groupby('condition')['correct_class'].apply(lambda x: np.mean(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.barplot(x='condition',\n",
    "            y='correct_class',\n",
    "           data=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.groupby('condition')['rt'].apply(lambda x: np.median(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X.groupby(['gameID','condition'])['correct'].apply(lambda x: np.mean(x))\n",
    "# X.groupby(['gameID'])['correct'].apply(lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## plot accuracy by condition and game (individual differences)\n",
    "#X.groupby(['gameID','condition'])['correct'].apply(lambda x: np.mean(x))\n",
    "\n",
    "## subset by full games only\n",
    "all_games = np.unique(X.gameID.values)\n",
    "full_games = [i for i in all_games if np.sum(X['gameID']==i)>50]\n",
    "_X = X[X['gameID'].isin(full_games)]\n",
    "\n",
    "game_acc_close = _X[_X['condition']=='closer'].groupby('gameID')['correct'].apply(lambda x: np.mean(x))\n",
    "game_acc_far = _X[_X['condition']=='further'].groupby('gameID')['correct'].apply(lambda x: np.mean(x))\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.scatter(game_acc_close,game_acc_far)\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.plot([0,1],[0,1],linestyle='dashed')\n",
    "plt.title('accuracy by condition and game')\n",
    "plt.xlabel('close accuracy')\n",
    "plt.ylabel('far accuracy')\n",
    "plt.savefig('./plots/accuracy_by_condition_and_game.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## overall accuracy by category\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "sns.barplot(x='target_category',y='correct',data=X)\n",
    "plt.xlabel('category')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy by category')\n",
    "plt.ylim([0,1])\n",
    "plt.savefig('./plots/accuracy_by_category.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objs = np.unique(X['target'].values)\n",
    "objs = [o for o in objs if o is not None]\n",
    "obj_acc_close = X[X['condition']=='closer'].groupby('target')['correct'].apply(lambda x: np.mean(x))\n",
    "obj_acc_far = X[X['condition']=='further'].groupby('target')['correct'].apply(lambda x: np.mean(x))\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.scatter(obj_acc_close,obj_acc_far)\n",
    "for i, txt in enumerate(objs):\n",
    "    plt.annotate(txt, (obj_acc_close[i],obj_acc_far[i]))\n",
    "plt.xlim([-0.1,1])\n",
    "plt.ylim([-0.1,1]) \n",
    "plt.plot([0,1],[0,1],linestyle='dashed')\n",
    "plt.xlabel('close accuracy')\n",
    "plt.ylabel('far accuracy')\n",
    "plt.title('accuracy by condition and object')\n",
    "plt.savefig('./plots/accuracy_by_condition_and_object.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objs = np.unique(X['target'].values)\n",
    "objs = [o for o in objs if o is not None]\n",
    "obj_acc_close = X[X['condition']=='closer'].groupby('target')['rt'].apply(lambda x: np.mean(x))\n",
    "obj_acc_far = X[X['condition']=='further'].groupby('target')['rt'].apply(lambda x: np.mean(x))\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.scatter(obj_acc_close,obj_acc_far)\n",
    "for i, txt in enumerate(objs):\n",
    "    plt.annotate(txt, (obj_acc_close[i],obj_acc_far[i]))\n",
    "plt.xlim([0,20000])\n",
    "plt.ylim([0,20000])\n",
    "plt.plot([0,20000],[0,20000],linestyle='dashed')\n",
    "plt.xlabel('close RT')\n",
    "plt.ylabel('far RT')\n",
    "plt.title('RT by condition and object')\n",
    "plt.savefig('./plots/RT_by_condition_and_object.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## subset by full games only\n",
    "all_games = np.unique(X.gameID.values)\n",
    "full_games = [i for i in all_games if np.sum(X['gameID']==i)>50]\n",
    "_X = X[X['gameID'].isin(full_games)]\n",
    "\n",
    "game_acc_close = _X[_X['condition']=='closer'].groupby('gameID')['rt'].apply(lambda x: np.median(x))\n",
    "game_acc_far = _X[_X['condition']=='further'].groupby('gameID')['rt'].apply(lambda x: np.median(x))\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.scatter(game_acc_close,game_acc_far)\n",
    "# plt.xlim([0,20000])\n",
    "# plt.ylim([0,20000])\n",
    "# plt.plot([0,20000],[0,20000],linestyle='dashed')\n",
    "plt.title('RT by condition and game')\n",
    "plt.xlabel('close RT')\n",
    "plt.ylabel('far RT')\n",
    "plt.savefig('./plots/RT_by_condition_and_game.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## subset by full games only\n",
    "all_games = np.unique(X.gameID.values)\n",
    "full_games = [i for i in all_games if np.sum(X['gameID']==i)>50]\n",
    "_X = X[X['gameID'].isin(full_games)]\n",
    "\n",
    "acc = _X.groupby('gameID')['correct'].apply(lambda x: np.mean(x))\n",
    "rt = _X.groupby('gameID')['rt'].apply(lambda x: np.mean(x))\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.scatter(acc,rt)\n",
    "plt.xlabel('accuracy')\n",
    "plt.ylabel('RT')\n",
    "plt.title('RT vs. accuracy by game')\n",
    "plt.savefig('./plots/RT_vs_accuracy_by_game.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Y = X.groupby(['target','condition'])['correct'].apply(lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import analysis_helpers as h\n",
    "reload(h)\n",
    "\n",
    "## get standardized object list\n",
    "categories = ['bird','car','chair','dog']\n",
    "obj_list = []\n",
    "for cat in categories:\n",
    "    for i,j in h.objcat.iteritems():\n",
    "        if j==cat:\n",
    "            obj_list.append(i)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### all sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## initialize confusion matrix\n",
    "confusion = np.zeros((len(obj_list),len(obj_list)))\n",
    "\n",
    "## generate confusion matrix by incrementing in each cell\n",
    "for i,d in X.iterrows():\n",
    "    targ_ind = obj_list.index(d['target'])\n",
    "    choice_ind = obj_list.index(d['choice'])\n",
    "    confusion[targ_ind,choice_ind] += 1\n",
    "    \n",
    "## normalized confusion matrix    \n",
    "normed = np.zeros((len(obj_list),len(obj_list)))\n",
    "for i in np.arange(len(confusion)):\n",
    "    normed[i,:] = confusion[i,:]/np.sum(confusion[i,:])    \n",
    "    \n",
    "## plot confusion matrix\n",
    "from matplotlib import cm\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(111)\n",
    "cax = ax.matshow(normed,vmin=0,vmax=1,cmap=cm.viridis)\n",
    "plt.xticks(range(len(normed)), obj_list, fontsize=12,rotation='vertical')\n",
    "plt.yticks(range(len(normed)), obj_list, fontsize=12)\n",
    "plt.colorbar(cax,shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./plots/confusion_matrix_all.pdf')\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### divided by condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conds = ['closer','further']\n",
    "\n",
    "for cond in conds:\n",
    "    ## initialize confusion matrix \n",
    "    confusion = np.zeros((len(obj_list),len(obj_list)))\n",
    "\n",
    "    _X = X[X['condition']==cond]\n",
    "    ## generate confusion matrix by incrementing in each cell\n",
    "    for i,d in _X.iterrows():\n",
    "        targ_ind = obj_list.index(d['target'])\n",
    "        choice_ind = obj_list.index(d['choice'])\n",
    "        confusion[targ_ind,choice_ind] += 1\n",
    "\n",
    "    ## normalized confusion matrix    \n",
    "    normed = np.zeros((len(obj_list),len(obj_list)))\n",
    "    for i in np.arange(len(confusion)):\n",
    "        normed[i,:] = confusion[i,:]/np.sum(confusion[i,:])    \n",
    "\n",
    "    ## plot confusion matrix\n",
    "    from matplotlib import cm\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = plt.subplot(111)\n",
    "    cax = ax.matshow(normed,vmin=0,vmax=1,cmap=cm.viridis)\n",
    "    plt.xticks(range(len(normed)), obj_list, fontsize=12,rotation='vertical')\n",
    "    plt.yticks(range(len(normed)), obj_list, fontsize=12)\n",
    "    plt.colorbar(cax,shrink=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./plots/confusion_matrix_{}.pdf'.format(cond))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot difference between close and far conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conds = ['closer','further']\n",
    "normed = np.zeros((len(obj_list),len(obj_list),2))\n",
    "\n",
    "for k,cond in enumerate(conds):\n",
    "    ## initialize confusion matrix \n",
    "    confusion = np.zeros((len(obj_list),len(obj_list)))\n",
    "\n",
    "    _X = X[X['condition']==cond]\n",
    "    ## generate confusion matrix by incrementing in each cell\n",
    "    for i,d in _X.iterrows():\n",
    "        targ_ind = obj_list.index(d['target'])\n",
    "        choice_ind = obj_list.index(d['choice'])\n",
    "        confusion[targ_ind,choice_ind] += 1\n",
    "\n",
    "    ## normalized confusion matrix    \n",
    "    for i in np.arange(len(confusion)):\n",
    "        normed[i,:,k] = confusion[i,:]/np.sum(confusion[i,:])    \n",
    "\n",
    "## plot difference in confusion matrix\n",
    "from matplotlib import cm\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(111)\n",
    "cax = ax.matshow(normed[:,:,0]-normed[:,:,1],vmin=-0.2,vmax=0.2,cmap=cm.BrBG)\n",
    "plt.xticks(range(len(normed)), obj_list, fontsize=12,rotation='vertical')\n",
    "plt.yticks(range(len(normed)), obj_list, fontsize=12)\n",
    "plt.colorbar(cax,shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./plots/confusion_matrix_close_minus_far.pdf')\n",
    "plt.close(fig)\n",
    "\n",
    "# save out to npy \n",
    "np.save('./human_confusion.npy',normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prep jsons for evaluation by passing forward to RSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format for json is a dictionary of dictionaries, where each top-level key refers to one of the renders, e.g. \"trial_20_cuckoo\". For each render, you can look up the similarity with each sketch, referenced with an abbreviated ID taken by trimming the last 12-character string, and appending an underscore, and the trial number. E.g., 'gameID_9903-d6e6a9ff-a878-4bee-b2d5-26e2e239460a_trial_9.npy' ==> '26e2e239460a_9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "## sample json paths\n",
    "json_path_prefix = '../models/refModule/json/'\n",
    "json_file = 'strict-similarity-pragmatics-fixedpose-augmented-splitbycontext_conv4_2.json'\n",
    "json_path = os.path.join(json_path_prefix,json_file)\n",
    "\n",
    "def load_json(json_path):\n",
    "    with open(json_path) as fp:\n",
    "        data = json.load(fp)  \n",
    "    return data\n",
    "\n",
    "## build dictionary to look up the appropriate render ID to use to associate with each sketch\n",
    "data = load_json(json_path)\n",
    "    \n",
    "obj_to_render = dict(zip([i.split('_')[-1] for i in data.keys()], data.keys()))  \n",
    "render_to_obj = dict(zip(data.keys(),[i.split('_')[-1] for i in data.keys()]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simplify_sketch(path): ## example path: 'gameID_9903-d6e6a9ff-a878-4bee-b2d5-26e2e239460a_trial_9.npy' ==> '26e2e239460a_9'\n",
    "    path = '_'.join(os.path.splitext(os.path.basename(path))[0].split('_')[1:])\n",
    "    path = path.split('-')[-1]\n",
    "    path = path.replace('_trial', '')\n",
    "    return path\n",
    "\n",
    "def add_simplified_ids(X):\n",
    "    ## add renderID and sketchID to dataframen\n",
    "    renderID = []\n",
    "    sketchID = []\n",
    "    for i,d in X.iterrows():\n",
    "        renderID.append(obj_to_render[d['target']])\n",
    "        sketchID.append(simplify_sketch(d['fname']))\n",
    "    X['renderID'] = renderID\n",
    "    X['sketchID'] = sketchID    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## add simplified id's\n",
    "X = add_simplified_ids(X)\n",
    "\n",
    "## save out again to csv with these additional columns added\n",
    "X.to_csv('./sketchpad_basic_recog_group_data_augmented.csv')\n",
    "\n",
    "## load in normalized confusion matrix\n",
    "confusion = np.load('./human_confusion.npy')\n",
    "close_con = confusion[:,:,0]\n",
    "far_con = confusion[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## get confusion matrix lookup table for sketches from each condition\n",
    "\n",
    "## close\n",
    "rows = [i for i in close_con]\n",
    "close_lookup = dict(zip(obj_list, [dict(zip(obj_list,r)) for r in rows])) \n",
    "\n",
    "## far\n",
    "rows = [i for i in far_con]\n",
    "far_lookup = dict(zip(obj_list, [dict(zip(obj_list,r)) for r in rows])) \n",
    "\n",
    "## list of 3d rendered objects\n",
    "render_list = data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## generate big json dictionary of dictionaries\n",
    "out_json = {}\n",
    "for i,this_render in enumerate(render_list):\n",
    "    print i, this_render\n",
    "    out_json[this_render] = {}\n",
    "    for i,d in X.iterrows():\n",
    "        if d['condition'] == 'closer':\n",
    "            which_dict = close_lookup\n",
    "        elif d['condition'] == 'further':\n",
    "            which_dict = far_lookup\n",
    "        this_sketch = d['sketchID']\n",
    "        this_target = d['target']\n",
    "        similarity = which_dict[this_target][render_to_obj[this_render]]\n",
    "        out_json[this_render][this_sketch] = similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## output json in the same format as the other similarity jsons\n",
    "output_path = '../models/refModule/json/similarity-human.json'\n",
    "with open(output_path, 'wb') as fp:\n",
    "    json.dump(out_json, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore how different measures of cost relate to similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## load in normalized confusion matrix\n",
    "confusion = np.load('./human_confusion.npy')\n",
    "close_con = confusion[:,:,0]\n",
    "far_con = confusion[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## define dictionaries from objects to the similarities\n",
    "sim_close = dict(zip(obj_list,np.diag(close_con)))\n",
    "sim_far = dict(zip(obj_list,np.diag(far_con)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## add human similarity btw sketch and target object as column to main csv\n",
    "human_similarity = []\n",
    "for i,d in X.iterrows():\n",
    "    if d['condition']=='closer':\n",
    "        human_similarity.append(sim_close[d['target']])\n",
    "    elif d['condition']=='further':\n",
    "        human_similarity.append(sim_far[d['target']])\n",
    "X['human_similarity'] = human_similarity   \n",
    "X.to_csv('./sketchpad_basic_recog_group_data_augmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## load augmented data and filter out outliers\n",
    "X = pd.read_csv('./sketchpad_basic_recog_group_data_augmented.csv')\n",
    "\n",
    "def remove_outliers(X,column):\n",
    "    mu = np.mean(X[column].values)\n",
    "    sd = np.std(X[column].values)\n",
    "    thresh = mu + 5*sd        \n",
    "    X = X.drop(X[X[column] > thresh].index)\n",
    "    return X\n",
    " \n",
    "X = remove_outliers(X,'drawDuration')    \n",
    "X = remove_outliers(X,'mean_intensity')\n",
    "X = remove_outliers(X,'num_strokes')\n",
    "\n",
    "print X.shape\n",
    "\n",
    "splits = ['splitbycontext','alldata']\n",
    "for split in splits:\n",
    "    ### subset drawing data csv by sketches that are accounted for here (i.e., that were not cost outliers)\n",
    "    D = pd.read_csv('../models/bdaInput/sketchData_fixedPose_{}_augmented2_pilot2.csv'.format(split))\n",
    "    print D.shape\n",
    "\n",
    "    remaining_sketches = list(np.unique(X['sketchID'].values))\n",
    "    _D = D[D['sketchLabel'].isin(remaining_sketches)]\n",
    "    print _D.shape\n",
    "    _D.to_csv('../models/bdaInput/sketchData_fixedPose_{}_augmented2_pilot2_costOutliersRemoved.csv'.format(split))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x,k=1,x0=0.5):\n",
    "    return 1 / (1 + np.exp(-k * (x - x0)))\n",
    "\n",
    "def add_rescaled_metric(X,metric,transform='maxnorm',k=5):\n",
    "    '''\n",
    "    input: X is a data frame, metric is the name of one of the (cost) metrics that you want to scale between 0 and 1\n",
    "            transform options include:\n",
    "                :'maxnorm', which means dividing each value by maximum in list\n",
    "                :'minmaxnorm', look at it\n",
    "                :'sigmoid', which means passing each value through logistic function with mean\n",
    "    output: X with additional column that has the rescaled metric\n",
    "    '''\n",
    "    if metric=='drawDuration': ## if handling drawDuration, log first -- no wait, maybe not \n",
    "#         vals = np.log(X[metric].values+1e-6)        \n",
    "        vals = X[metric].values\n",
    "    else:\n",
    "        vals = X[metric].values\n",
    "    X['vals'] = vals\n",
    "    if transform=='maxnorm':\n",
    "        top_val = np.max(vals)\n",
    "        rescaled_val = []\n",
    "        for i,d in X.iterrows():\n",
    "            rescaled_val.append(d['vals']/top_val)\n",
    "    elif transform=='minmaxnorm':\n",
    "        bottom_val = np.min(vals)\n",
    "        top_val = np.max(vals)\n",
    "        rescaled_val = []\n",
    "        for i,d in X.iterrows():\n",
    "            rescaled_val.append((d['vals']-bottom_val)/(top_val-bottom_val))        \n",
    "    elif transform=='sigmoid':\n",
    "        median_val = np.median(vals)\n",
    "        rescaled_val = []\n",
    "        for i,d in X.iterrows():\n",
    "            rescaled_val.append(sigmoid(d['vals'],k=k,x0=median_val))\n",
    "    X['rescaled_{}'.format(metric)] = rescaled_val          \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = add_rescaled_metric(X,'num_strokes',transform='minmaxnorm')\n",
    "X = add_rescaled_metric(X,'mean_intensity',transform='minmaxnorm')\n",
    "X = add_rescaled_metric(X,'drawDuration',transform='minmaxnorm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost_metric = 'rescaled_drawDuration' ## cost metrics options: 'num_strokes','mean_intensity','drawDuration'\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "sns.regplot(data=X,\n",
    "           x='human_similarity', ## similarity\n",
    "           y=cost_metric,\n",
    "           robust=False,\n",
    "           ci=None,\n",
    "           logistic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = 'rescaled_num_strokes'\n",
    "f = X[X['condition']=='further'][metric].values\n",
    "c = X[X['condition']=='closer'][metric].values\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "plt.subplot(121)\n",
    "h = sns.distplot(f,color=[0.8,0.2,0.2],label='far')\n",
    "h = sns.distplot(c,color=[0.2,0.2,0.8],label='close')\n",
    "plt.ylim([0,6])\n",
    "plt.legend()\n",
    "plt.xlim([0,1])\n",
    "plt.xlabel(metric.split('_')[1])\n",
    "plt.ylabel('density')\n",
    "plt.savefig('./plots/cost_density_by_condition_{}.pdf'.format(metric.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## resave out csv, augmented with rescaled cost parameters\n",
    "X.to_csv('./sketchpad_basic_recog_group_data_augmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(131)\n",
    "plt.scatter(X['rescaled_mean_intensity'],X['rescaled_drawDuration'])\n",
    "plt.subplot(132)\n",
    "plt.scatter(X['rescaled_num_strokes'],X['rescaled_drawDuration'])\n",
    "plt.subplot(133)\n",
    "plt.scatter(X['rescaled_mean_intensity'],X['rescaled_num_strokes'])\n",
    "\n",
    "print stats.pearsonr(X['rescaled_mean_intensity'],X['rescaled_drawDuration'])\n",
    "print stats.pearsonr(X['rescaled_num_strokes'],X['rescaled_drawDuration'])\n",
    "print stats.pearsonr(X['rescaled_mean_intensity'],X['rescaled_num_strokes'])\n",
    "\n",
    "print stats.pearsonr(X['rescaled_mean_intensity'],X['human_similarity'])\n",
    "print stats.pearsonr(X['rescaled_num_strokes'],X['human_similarity'])\n",
    "print stats.pearsonr(X['rescaled_drawDuration'],X['human_similarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate cost dictionaries to try out with pragmatics model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## generate cost dictionaries to try out with pragmatics model\n",
    "print len(np.unique(X['sketchID'].values))\n",
    "sketchID_list = np.unique(X['sketchID'].values)\n",
    "metrics = ['drawDuration','mean_intensity','num_strokes']\n",
    "\n",
    "for metric in metrics:    \n",
    "    print metric\n",
    "    #### draw duration\n",
    "    cost_json = {}\n",
    "    for i,d in enumerate(sketchID_list):\n",
    "        assert len(np.unique(X[X['sketchID']==d]['rescaled_{}'.format(metric)].values))==1\n",
    "        cost_json[d] = X[X['sketchID']==d]['rescaled_{}'.format(metric)].values[0]    \n",
    "\n",
    "    ## output json in the same format as the other cost json\n",
    "    output_path = '../models/refModule/json/costs-fixedPose96-{}.json'.format(metric)\n",
    "    with open(output_path, 'wb') as fp:\n",
    "        json.dump(cost_json, fp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = load_json('../models/refModule/json/costs-fixedPose96-drawDuration.json')\n",
    "print len(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate condition-lookup.json to be able to pair sketches with condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## generate condition-lookup.json to be able to pair sketches with condition\n",
    "cond_json = {}\n",
    "sketchID_list = np.unique(X['sketchID'].values)\n",
    "for i,d in enumerate(sketchID_list):\n",
    "    cond = X[X['sketchID']==d]['condition'].values[0]\n",
    "    obj = X[X['sketchID']==d]['target'].values[0]\n",
    "    cond_json[d] = '{}_{}'.format(cond,obj)\n",
    "    \n",
    "## output json in the same format as the other cost json\n",
    "output_path = '../models/bdaInput/condition-lookup.json'\n",
    "with open(output_path, 'wb') as fp:\n",
    "    json.dump(cond_json, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MAP param values from bdaOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_bdaoutput = '../models/bdaOutput'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "marp = 'human_combined_cost_alldataParams.csv'\n",
    "x = pd.read_csv(os.path.join(path_to_bdaoutput,marp))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "marp = 'human_S0_cost_alldataParams.csv'\n",
    "x = pd.read_csv(os.path.join(path_to_bdaoutput,marp))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "marp = 'human_combined_nocost_alldataParams.csv'\n",
    "x = pd.read_csv(os.path.join(path_to_bdaoutput,marp))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "marp = 'human_S0_nocost_alldataParams.csv'\n",
    "x = pd.read_csv(os.path.join(path_to_bdaoutput,marp))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore model predictions and do model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## helpers\n",
    "def sumlogprob(a,b):\n",
    "    if (a > b):\n",
    "        return a + np.log1p(np.exp(b-a))\n",
    "    else:\n",
    "        return b + np.log1p(np.exp(a-b))  \n",
    "    \n",
    "dogs = sorted(['weimaraner', 'chihuahua', 'basset', 'doberman', 'bloodhound', 'bullmastiff', 'goldenretriever', 'pug'])\n",
    "chairs = sorted(['leather', 'straight', 'squat', 'sling', 'woven', 'waiting', 'inlay','knob'])\n",
    "birds = sorted(['crow', 'pigeon', 'robin', 'sparrow', 'tomtit', 'nightingale', 'bluejay', 'cuckoo'])\n",
    "cars = sorted(['beetle', 'bluesport', 'brown', 'white', 'redsport', 'redantique', 'hatchback', 'bluesedan'])\n",
    "\n",
    "def flatten(x):\n",
    "    return [item for sublist in x for item in sublist]\n",
    "\n",
    "def make_category_by_obj_palette():\n",
    "    import itertools\n",
    "    col = []\n",
    "    for j in sns.color_palette(\"hls\", 4):\n",
    "        col.append([i for i in itertools.repeat(j, 8)])\n",
    "    return flatten(col)\n",
    "\n",
    "colors = sns.color_palette(\"Dark2\")\n",
    "sns.palplot(sns.color_palette(\"Dark2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define relevant paths\n",
    "path_to_bda_output = '../models/bdaOutput'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### which model do you want to visualize? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## define menu of model options\n",
    "zoo = ['pragmatic_cost','literal_cost','pragmatic_nocost','literal_nocost']\n",
    "\n",
    "predFiles = ['human_combined_costPredictives.csv','human_S0_costPredictives.csv',\\\n",
    "        'human_combined_nocostPredictives.csv','human_S0_nocostPredictives.csv']\n",
    "\n",
    "model_dict = dict(zip(zoo,predFiles))\n",
    "\n",
    "## which model do you want to make visualizations for?\n",
    "this_model = 'pragmatic_cost'\n",
    "\n",
    "## load in predictives for this model\n",
    "predictives = pd.read_csv(os.path.join(path_to_bda_output,model_dict[this_model]))\n",
    "\n",
    "### various preprocessing\n",
    "predictives = predictives.drop(columns=['trueSketch'],axis=1) ## remove some columns\n",
    "predictives = predictives.rename(index=str,columns={'coarseGrainedTrueSketch':'trueSketch','coarseGrainedPossibleSketch':'predSketch'})\n",
    "predictives['trueCondition'], predictives['trueSketchTarget'] = predictives['trueSketch'].str.split('_', 1).str\n",
    "predictives['predCondition'], predictives['predSketchTarget'] = predictives['predSketch'].str.split('_', 1).str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coarse aggregation: higher probability assigned to true condition than false?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PP = predictives.groupby(['trueSketch', 'trueCondition', 'predCondition'])['modelProb'].\\\n",
    "apply(lambda x: reduce(sumlogprob,x) - np.log(len(x))).\\\n",
    "groupby(['trueCondition','predCondition']).\\\n",
    "apply(lambda x: np.sum(np.exp(x)))\n",
    "\n",
    "## plot it\n",
    "colors = sns.color_palette(\"Dark2\")\n",
    "sns.set_context('poster')\n",
    "Y = pd.DataFrame(PP)\n",
    "Y.reset_index(inplace=True)\n",
    "g = sns.factorplot(x=\"trueCondition\", \n",
    "                   y=\"modelProb\", \n",
    "                   hue=\"predCondition\", \n",
    "                   data=Y,\n",
    "                   size=6, kind=\"bar\", palette=colors)\n",
    "plt.ylim([0,1])\n",
    "plt.axhline(0.5,linestyle=':',color='k')\n",
    "plt.ylabel('model probability',fontsize=24)\n",
    "plt.xlabel('true condition',fontsize=24)\n",
    "plt.savefig('./plots/context_effect_by_condition_bars_{}.pdf'.format(this_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PP = predictives.groupby(['trueSketch', 'predSketch'])['modelProb'].\\\n",
    "apply(lambda x: reduce(sumlogprob,x) - np.log(len(x))).\\\n",
    "groupby(['trueSketch','predSketch']).\\\n",
    "apply(lambda x: np.sum(np.exp(x)))\n",
    "PP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PP = pd.DataFrame(PP)\n",
    "PP.reset_index(inplace=True)\n",
    "print PP[PP['trueSketch']=='closer_basset']['modelProb'].sum() ## should be close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "close_pref = []\n",
    "far_pref = []\n",
    "congruent_pref = []\n",
    "for i,obj in enumerate(obj_list):\n",
    "\n",
    "    ## TCPC = \"true close; predicted close\"\n",
    "    ## TCPF = \"true close; predicted far\"\n",
    "    ## TFPF = \"true far; predicted far\"\n",
    "    ## TFPC = \"true far: predicted close\"\n",
    "    TCPC = PP[(PP['trueSketch']=='closer_{}'.format(obj)) &\\\n",
    "              (PP['predSketch']=='closer_{}'.format(obj))]['modelProb'].values[0]\n",
    "    TCPF = PP[(PP['trueSketch']=='closer_{}'.format(obj)) &\\\n",
    "              (PP['predSketch']=='further_{}'.format(obj))]['modelProb'].values[0]\n",
    "    TFPF = PP[(PP['trueSketch']=='further_{}'.format(obj)) &\\\n",
    "              (PP['predSketch']=='further_{}'.format(obj))]['modelProb'].values[0]\n",
    "    TFPC = PP[(PP['trueSketch']=='further_{}'.format(obj)) &\\\n",
    "              (PP['predSketch']=='closer_{}'.format(obj))]['modelProb'].values[0]\n",
    "\n",
    "    close_pref.append(np.log(TCPC) - np.log(TCPF))\n",
    "    far_pref.append(np.log(TFPF) - np.log(TFPC))\n",
    "    congruent_pref.append(np.log(TCPC) - np.log(TCPF) + np.log(TFPF) - np.log(TFPC))\n",
    "#     close_pref.append(TCPC/TCPF)\n",
    "#     far_pref.append(TFPF/TFPC)\n",
    "#     congruent_pref.append((TCPC*TFPF)/(TCPF*TFPC))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B = pd.DataFrame([close_pref,far_pref,congruent_pref,obj_list])\n",
    "B = B.transpose()\n",
    "B.columns = ['close_pref','far_pref','congruent_pref','object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=B,\n",
    "            x='object',\n",
    "            y='close_pref',\n",
    "            palette=colors32)\n",
    "h = plt.xticks(range(len(obj_list)), obj_list, fontsize=16,rotation='vertical')\n",
    "plt.ylim([-6,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate 4096x7ish dataframe with log probabilities already aggregated at course condition&object level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## get dataframe with probability assigned to every sketch category, for each sketch category\n",
    "P = predictives.groupby(['trueSketch','predSketch'])['modelProb'].\\\n",
    "apply(lambda x: reduce(sumlogprob,x) - np.log(len(x)))\n",
    "\n",
    "## generate 4096x7ish dataframe with log probabilities already aggregated at course condition&object level\n",
    "P = pd.DataFrame(P)\n",
    "P.reset_index(inplace=True)\n",
    "P['trueCondition'], P['trueSketchTarget'] = P['trueSketch'].str.split('_', 1).str\n",
    "P['predCondition'], P['predSketchTarget'] = P['predSketch'].str.split('_', 1).str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate proportion of objects for which model able to predict correct condition+obj sketch category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## calculate proportion of objects for which model able to predict correct condition+obj sketch category\n",
    "## i.e., put in top k, for various values of k\n",
    "from __future__ import division\n",
    "objs = np.unique(P['trueSketchTarget'].values)\n",
    "conds = np.unique(P['trueCondition'].values)\n",
    "\n",
    "Counter = []\n",
    "Conds = []\n",
    "K = []\n",
    "for cond in conds:\n",
    "    print 'Analyzing {}'.format(cond)\n",
    "    for k in np.arange(1,64):\n",
    "        counter = 0\n",
    "        for obj in objs:    \n",
    "            Q = P[(P['trueSketchTarget']==obj) & (P['trueCondition']==cond)].\\\n",
    "            nlargest(k,'modelProb')[['trueSketch','predSketch','trueCondition','modelProb']]  \n",
    "            counter += np.sum(Q['predSketch']==Q['trueSketch'])\n",
    "        Counter.append(counter/len(objs))\n",
    "        Conds.append(cond)\n",
    "        K.append(k)\n",
    "\n",
    "## make dataframe         \n",
    "D = pd.DataFrame([Counter,Conds,K])\n",
    "D = D.transpose()\n",
    "D.columns = ['prop','condition','k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## plot it\n",
    "krange = 15 ## how many values of k to plot\n",
    "sns.set_context('talk')\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "sns.pointplot(x='k',y='prop',hue='condition',data=D,markers='.',palette=colors)\n",
    "plt.xlim([0,krange])\n",
    "plt.ylabel('proportion',fontsize=24)\n",
    "plt.xlabel('k',fontsize=24)\n",
    "plt.title('proportion of objects for which model predicts correct sketch type within top k')\n",
    "plt.ylim([0,1.05])\n",
    "plt.savefig('./plots/proportion_correct_sketch_type_in_top_k_{}.pdf'.format(this_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## plots from recognition experiment -- close vs. far accuracy gap, and then by obj or category\n",
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###todo: loglikelihood plot, coarse by context x condition, then by object, then the k plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### next: use some of this stuff to train embedding, also make predictions about viewer so we can\n",
    "## effectively fit all behavior in this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
