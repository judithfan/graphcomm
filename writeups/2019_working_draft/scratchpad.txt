% The social inference module depends on two sources of information in order to decide which sketch would be most communicatively useful to produce in context: (1) the perceptual correspondence between each object in context and any sketch it could produce, where the set of producible sketches includes all sketches in the test set (i.e., not used to train the visual encoder) and (2) the cost of producing those sketches, operationalized as the amount of time taken to produce it. Using this information, it outputs a distribution of scores over all test-set sketches, where the scores reflect each sketch's relative communicative utility.

% We formalized the problem of how to produce sketches that are informative in context in terms of two components: a \textit{visual encoder} module, which is a function taking a sketch and object rendering as input and returns how well the sketch corresponds to the object, and a \textit{social inference} module, another function that evaluates the degree to which different sketches would inform the viewer about the identity of the target object in context without being too costly to produce, and returns a distribution of scores over possible sketches. 

% Consistent with prior work \cite{FanCommon2018}, we hypothesized that the best-fitting visual encoder module would use the abstract features learned by modern deep convolutional neural networks pre-trained on natural object recognition tasks, rather than lower-level image statistics. 
% Also in line with prior work \cite{goodman2016pragmatic}, we hypothesized that the best-fitting probabilistic social inference module would possess both context and cost sensitivity. 

% perform in assigning a high rank to the target sketch category in context? Supporting our hypothesis that sketch-object perceptual correspondences provide a strong basis for modeling contextual flexibility, we found that directly using empirical measurements of perceptual sketch-object correspondences led to the best overall sketch retrieval performance (Human-Context-Cost: X.XX, Human-NoContext-Cost: X.XX, Human-Context-NoCost: X.XX). Relative to this benchmark, we found that visual encoder modules using higher-level features generally succeeded in assigning a high score to the congruent sketch category, as measured by the rank of the target sketch category (High-Context-Cost: X.XX, Mid-Context-Cost: X.XX), whereas using low-level features adapted from an early layer of the convnet yielded poor performance (Low-Context-Cost: X.XX).

% Next, to test the influence of perceptual representations, we compared our empirically measured similarity values with three alternative similarity functions lightly adapted from different layers of a convolutional neural network (see \emph{Materials and Methods}, Eq. 1) \mwu{maybe mention why we want to try different layers}. Critically, because these are \emph{encoding} models that take raw visual input, they can generalize to entirely novel sketches. One of the most striking aspects of visual communication is the human ability to recognize an object from a handful of strokes, even though the respective images have little to no overlap in low-level visual features. Motivated by prior work \cite{FanCommon2018}, we hypothesized that performance in visual communication would improve as we ascend the layers of a visual encoding model \mwu{a sentence talking about how visual encoding change in VGG might help build intuition.}. We augmented a pre-trained \mwu{what does pre-training mean -- define} deep convolutional neural network architecture with a shallow \emph{adaptor} network that was trained on the data from our matching task and learned to evaluate the perceptual correspondence between drawings and photos.

% We conducted another Bayesian Data Analysis comparing feature vectors adapted from \emph{pool1}, \emph{conv42}, and \emph{fc6}, respectively, as our raw similarity function. We found that the sketcher adapted from the later layer of \emph{fc6} performed better than the mid-layer of \emph{conv42} ($BF= XX$, which in turn performed better than \emph{pool1} ($BF = XX$; see Fig. \ref{model_results}B). \mwu{in these paragraphs, good to give written intuition on what these results imply.}


%% report the comparisons between high & medium, medium & low, high & low here:
% BF evidence for fc6 model vs. conv42: 5.7 x 10^97
% BF evidence for fc6 model vs. pool1: 4.7 x 10^274

%% report additional checks on pragmatics for high perception model
% BF evidence for pragmatics (within fc6): 7.1 x 10^34
% BF evidence for *cost* (within fc6): 48.6

% \subsubsection*{Evaluating model predictions}

% set up basic evaluation dimensions
% Having established the contributions of visual abstraction, context sensitivity, and cost sensitivity to the ability of the full model generalize to unseen contexts and sketches, we sought to gain deeper insight into the model's pattern of behavior in the communication task. 

% Here we compare how well the full model, and its lesioned variants, on three task dimensions: (1) sketch retrieval: the ability to assign a high absolute rank to the target sketch category in context, selecting among all objects and context categories; (2) context congruity: the ability to consistently prefer the context-congruent version of the target object (i.e., a close sketch of the target on a close trial); (3) cost modulation: a consistent preference for more costly sketches on close trials and less costly sketches on far trials, mirroring human behavior.

%% report absolute rank performance

% How well did each variant of the model perform in assigning a high rank to the target sketch category in context? Supporting our hypothesis that sketch-object perceptual correspondences provide a strong basis for modeling contextual flexibility, we found that directly using empirical measurements of perceptual sketch-object correspondences led to the best overall sketch retrieval performance (Human-Context-Cost: X.XX, Human-NoContext-Cost: X.XX, Human-Context-NoCost: X.XX). Relative to this benchmark, we found that visual encoder modules using higher-level features generally succeeded in assigning a high score to the congruent sketch category, as measured by the rank of the target sketch category (High-Context-Cost: X.XX, Mid-Context-Cost: X.XX), whereas using low-level features adapted from an early layer of the convnet yielded poor performance (Low-Context-Cost: X.XX).

% =======
% How well did each variant of the model perform in assigning a high rank to the target sketch category in context? Supporting our hypothesis that sketch-object perceptual correspondences provide a strong basis for modeling contextual flexibility, we found that directly using empirical measurements of perceptual sketch-object correspondences led to the best overall sketch retrieval performance (Human-Context-Cost: X.XX, Human-NoContext-Cost: X.XX, Human-Context-NoCost: X.XX). Relative to this benchmark, we found that visual encoder modules using more abstract features generally succeeded in assigning a high score to the congruent sketch category, as measured by the rank of the target sketch category (High-Context-Cost: X.XX, Mid-Context-Cost: X.XX), whereas using low-level features adapted from an early layer of the convnet yielded poor performance (Low-Context-Cost: X.XX). \mwu{a common metric in ML is \% of time to the correct rank is in the top N (retrieveal @ N).}
% >>>>>>> d20c7137bd7e724a32eb08ee4ddf3f93a09dc202

%% report congruent context preference
% Only models using high-level visual features (i.e., empirically estimated or adapted from a higher layer of VGG) and were sensitive to both context and cost succeeded in selecting the congruent sketch category (Empirical-Context-Cost: XX.X\%, 95\% CI: [XX.X\%,XX.X\%]; High-Context-Cost: XX.X\%, 95\% CI: [XX.X\%,XX.X\%]). On the other hand, models that were insensitive to either context or cost failed to select the congruent sketch category (High-NoContext-Cost: XX.X\%, 95\% CI: [XX.X\%,XX.X\%]; High-Context-NoCost: XX.X\%, 95\% CI: [XX.X\%,XX.X\%]; Mid-Context-Cost: XX.X\% 95\% CI: [XX.X\%,XX.X\%]; Low-Context-Cost: XX.X\%, 95\% CI: [XX.X\%,XX.X\%]).

%% report cost behavior
% We found that human participants strongly modulated the cost of the sketches by context condition, producing costlier sketches in the close condition, and less costly sketches in the far condition. Next we sought to evaluate the extent to which each model exhibited this pattern of behavior (Fig.~\ref{model_results}D). We made three main observations: (1) Models that were insensitive to context (i.e., Human-NoContext-Cost, High-NoContext-Cost) or used lower-level visual features (i.e., Mid-Context-Cost, Low-Context-Cost) did not modulate cost by context condition, and generally produced sketches that were less costly than average in both conditions (XX on difference between conditions, XX on being below average). (2) Models equipped with `visual abstraction' (i.e., employed empirical perceptual measurements or higher-level visual features), were sensitive to context, but insensitive to cost \textit{did} modulate cost by context condition (i.e., Human-Context-NoCost, High-Context-NoCost), but generally produced sketches that were costlier than average in both conditions. This cost modulation reflects the overall higher sketch-object perceptual correspondences exhibited by close sketches compared with far sketches, such that these context-sensitive models tend to produce costlier, higher-fidelity sketches in the close condition than in the far condition, on which the less costly, lower-fidelity sketches can still distinguish the target from distractors. (3) Only models equipped with visual abstraction and were both context and cost sensitive modulated cost by context condition (i.e., Human-Context-Cost), and displayed a consistent preference for costlier sketches than average in the close condition, and less costly sketches than average in the far condition (XX on the difference between close and far, and XX on the differences with respect to the average), mirroring human behavior. 

% =======
% We found that human participants strongly modulated the cost of the sketches by context condition, producing costlier sketches in the close condition, and less costly sketches in the far condition. Next we sought to evaluate the extent to which each model exhibited this pattern of behavior (Fig.~\ref{model_results}D). We made three main observations: (1) Models that were insensitive to context (i.e., Human-NoContext-Cost, High-NoContext-Cost) or used lower-level visual features (i.e., Mid-Context-Cost, Low-Context-Cost) did not modulate cost by context condition, and generally produced sketches that were less costly than average in both conditions (XX on difference between conditions, XX on being below average). (2) Models equipped with `visual abstraction' (i.e., employed empirical perceptual measurements or higher-level visual features), were sensitive to context, but insensitive to cost \texti{did} modulate cost by context condition (i.e., Human-Context-NoCost, High-Context-NoCost), but generally produced sketches that were costlier than average in both conditions. \mwu{this paragrpah is a lot; i read it a few times and didnt get it.} This cost modulation reflects the overall higher sketch-object perceptual correspondences exhibited by close sketches compared with far sketches, such that these context-sensitive models tend to produce costlier, higher-fidelity sketches in the close condition than in the far condition, on which the less costly, lower-fidelity sketches can still distinguish the target from distractors. (3) Only models equipped with visual abstraction and were both context and cost sensitive modulated cost by context condition (i.e., Human-Context-Cost), and displayed a consistent preference for costlier sketches than average in the close condition, and less costly sketches than average in the far condition (XX on the difference between close and far, and XX on the differences with respect to the average), mirroring human behavior.
% >>>>>>> d20c7137bd7e724a32eb08ee4ddf3f93a09dc202