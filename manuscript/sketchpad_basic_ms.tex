%% Documentclass:
\documentclass[manuscript]{stjour}

%% Manuscript, for double spaced, larger fonts
% \documentclass[manuscript]{stjour}
%% Only needed if you use `manuscript' option
\journalname{Open Mind}
%\journalname{Computational Psychiatry}


%%%%%%%%%%% Please supply information %%%%%%%%%%%%%%%%%%%%%%%%%

%% For Open Mind:
%% Supplementary Materials:
% \supplementslinks{dx.doi.org/10.1098/rsif.2013.0969}

%% For Computational Psychiatry
% \supportinginfo{dx.doi.org/10.7910/DVN/PQ6ILM}

%% If no conflicts, this command doesn't need to be used
%% \conflictsofinterest{}

%%%%%%%%%%% to be supplied by MIT Press, only %%%%%%%%%%%%%%%%%

\received{Day Month Year}
\accepted{Day Month Year}
\published{Day Month Year}

%% DOI address:
\setdoi{10.1098/rsif.2013.0969}

%%%%%%%% End MIT Press commands %%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% author definitions should be placed here:

%% example definition
\def\taupav{\tau_{\mathrm{Pav}}}

\begin{document}
\title{Contextual flexibility in visual communication}

\author[Author Names]
{Anonymous}

% \author[Author Names]
% {Judith E. Fan \affil{1,*}, Robert X.D. Hawkins \affil{1,*}, Mike Wu \affil{2}, \and Noah D. Goodman \affil{1,2}}

% \affiliation{1}{Department of Psychology, Stanford University, Stanford, CA 94305}
% \affiliation{2}{Department of Computer Science, Stanford University, Stanford, CA 94305}
% \affiliation{*}{These authors contributed equally to this work.}

% \correspondingauthor{Judith E. Fan}{jefan@stanford.edu}

\keywords{drawing, deep learning, pragmatics, computational modeling, Rational Speech Act framework}

\begin{abstract}
Communication is central to the success of our species: it allows us to learn from each other, coordinate our actions, and express otherwise hidden thoughts. Critically, human communication goes beyond language production --- humans also express their ideas in visual form. Visual communication lies at the heart of key innovations, and forms the foundation for the cultural transmission of knowledge and higher-level reasoning. This paper examines drawing, the most basic form of visual communication. Communicative uses of drawing pose a core challenge for theories of vision and communication alike: they require a detailed understanding of how sensory information is encoded and how social context guides what information is relevant to communicate. Our computational modeling approach addresses this challenge by combining a high-performing computational model of vision with a formal Bayesian model of social reasoning during communication in order to explain how people flexibly adapt their drawings according to the current context. We employ drawing-based reference games throughout our modeling and experimental work. These reference games involve two players: a \textit{sketcher} who aims to help a \textit{viewer} pick out a target object from an array of alternative, distractor objects by representing it in a drawing. We found that people exploit information in common ground with their partner to efficiently communicate, and that this contextual flexibility was well explained by our computational model. In the long run, understanding the computational basis of visual communication may shed light on the nature of human visual abstraction and the emergence of graphical conventions.

\end{abstract}

% Communication is not limited to verbal language; humans can make use of many different tools to convey meaning to a partner. While theories of communication often rely upon the existence of modality-general pragmatic reasoning mechanisms, these predictions have rarely been tested outside the verbal modality. In this experiment, we aim to test how context -- the set of potential referents in common ground -- affects visual communication: using drawings to convey the identity of rich naturalistic images. Past work in the verbal modality has shown a strong influence of speaker informativity: when the intended referent is surrounded by very similar objects, speakers send more detailed and verbose messages to help the listener distinguish it. We expect analogous results to hold in the visual modality. For instance, when referring to a car in a context containing dogs and birds, speakers should use simpler, more abstract drawings than when referring to the same object in the context of other cars.

\section*{Introduction}

% high level
Communication is central to the success of our species: it allows us to learn from each other, coordinate our actions, and express otherwise hidden thoughts. Critically, human communication goes beyond language production --- humans also express their ideas in visual form. Visual communication lies at the heart of key innovations, and forms the foundation for the cultural transmission of knowledge and higher-level reasoning. Drawing is perhaps the most direct and basic form of visual communication. We perceive drawings of objects as resembling physical objects in spite of the fact that drawings and objects are profoundly different in composition.

In the appropriate context, even a few strokes can express the identity of a face \cite[]{bergmann2013impact}, a suggested route \cite[]{agrawala2001rendering}, or an intention to act \cite[]{Galantucci:2005uh}. Conventionalized representations such as maps, graphs, and diagrams can transmit high-dimensional data and convey complex ideas by recombining relatively few geometric primitives, such as boxes, lines, arrows, ellipses \cite[]{tversky2000lines}. Despite the ubiquity and importance of such visualizations, we lack a unified computational theory of how human perception, action, and social inference are integrated to support flexible visual communication.

Communicative uses of drawing pose a core challenge for theories of vision and communication alike: they require a detailed understanding of how sensory information is encoded, how social context guides what information is relevant to communicate, and estimating the costs of visual production. In this paper, we meet this challenge by combining high-performing models of sensory representation from deep learning with insights from Bayesian cognitive models of social reasoning in language. 

There is now abundant and converging evidence that deep convolutional neural networks (DCNNs) optimized to recognize objects in natural photographs can learn general-purpose feature representations that support a wide variety of visual tasks, including but not limited to object categorization, segmentation, and pose/size estimation \cite[]{hong2016explicit,he2017mask}.

% recent successes learning general visual features, but these inherently acontextual
In particular, despite the fact that drawings are highly abstracted away from natural visual inputs, a relatively modest amount of `fine-tuning' of models previously trained only on photos has been shown to lead to human-level performance on sketch categorization \cite[]{yu2015sketch}, as well as vast improvements in discerning image-level correspondences between sketches and photos \cite[]{sangkloy2016sketchy}. Even without fine-tuning, prior work has shown a striking isomorphism in the representations of object categories in drawings and photos at higher layers in these models \cite[]{fan2015common}. As such, models from this architecture class are a strong candidate for an encoder that can support the level visual abstraction required for representing abstract information in drawings.

However, these visual feature representations are inherently acontextual, and cannot explain why people might use one drawing to communicate an idea in one context (e.g., XX), and a different drawing in another context (e.g., XX). Rational Speech Act (RSA) models provide a probabilistic framework for deriving communicative flexibility from general principles of social cognition \cite{GoodmanFrank16_RSATiCS}. These models draw on ideas and insights from decision theory, probabilistic models of cognition, bounded rationality, and linguistics, taking particular inspiration from the insights of Paul Grice (\citeyear{Grice75_LogicConversation}), who provided a philosophical framework for understanding how natural language use reflects exquisite social reasoning in context. Gricean listeners assume speakers are cooperative, choosing appropriately informative utterances to convey particular meanings. Under this assumption, listeners attempt to infer the speaker's intended communicative goal, working backwards from the form of the utterance. RSA models formalizing this core idea in terms of Bayesian inference have provided unified explanations for a variety of complex linguistic patterns, achieved good quantitative fits with experimental data, and been incorporated into artificial agents that communicate with each other to solve real-world tasks.

Previous work has shown that RSA models account for context sensitivity in human speakers \cite[]{GrafEtAl16_BasicLevel}. For instance, if a speaker needs to refer to a Dalmatian that appears alongside a poodle and a bear, the model predicts that this speaker will prefer the more specific utterance (``Dalmatian'') to the shorter and lower-cost basic-level utterance (``dog''). This is because the literal meaning of the word ``dog'' fits both the Dalmatian and poodle, whereas the word ``Dalmatian'' only fits the Dalmatian. Consequently, the speaker model reasons that ``Dalmatian'' is more informative. By using the word ``Dalmatian'', the speaker encourages the listener to infer the single correct world state (i.e., the target is the Dalmatian) instead of allowing two world states to seem probable, one of which is incorrect (i.e., the target is the poodle). In a context where there is only one dog, however, the speaker will prefer to say ``dog'', which is as informative as ``Dalmatian'' but is less costly to produce. 

\section*{Methods}

\subsection*{Participants}

A total of 206 unique participants, who were recruited via Amazon Mechanical Turk (AMT) and grouped into pairs, completed the experiment. 3 pairs were excluded because the sketcher in these pairs did not follow instructions, having annotated their sketches with text. 

\subsection*{Stimuli}

Stimuli were 32 3D mesh models of objects belonging to 4 categories (i.e., birds, chairs, cars, dogs), containing eight objects each. 40 color images of each object were produced by rendering it from a 10$^{\circ}$ viewing angle (i.e., slightly above) at a fixed distance on a gray background, each rotated by an additional 9$^{\circ}$ about the vertical axis. 

\subsection*{Task}

Drawings were collected in the context of an online, sketching-based reference game (``Guess My Sketch!''). The game involved two players: a \textit{sketcher} who aims to help a \textit{viewer} pick out a target object from an array of alternative, distractor objects by representing it in a sketch. On each trial, both participants were shown an array of the same four objects; however, the positions of these objects were randomized for each participant. On each trial, one of the four objects was highlighted on the sketcher's screen to designate it as the target. 

Sketchers drew using black ink on digital canvas (pen width = 5 pixels; 300 x 300 pixels) embedded in a web browser window using Paper.js (http://paperjs.org/). Participants drew using the mouse cursor, and were not able to delete previous strokes. Each stroke of which was rendered on the viewer's screen immediately upon the completion of each stroke. There were no restrictions on how long participants could take to make their drawings. After clicking a submit button, the viewer guessed the identity of the drawn object by clicking one of the four objects in the array. Otherwise, the viewer had no other means of communicating with the sketcher. Both participants received immediate task-related feedback: the sketcher learned which object the viewer had clicked, and the viewer learned the identity of the target. Both participants earned bonus points for each correct response. 

For each pair, objects were grouped into eight quartets: Four of these quartets contained objects from the same category (``close'' condition); the other four of these quartets contained objects from different categories (``far'' condition). Each quartet was presented four times, such that each object in the quartet served as the target exactly once. The assignment of objects to quartet and condition was randomized across pairs. 

\subsection*{Modeling}

\subsubsection*{Visual encoder module}

Features were extracted using VGG-19 \cite{simonyan2014very}, a high-performing deep convolutional neural network model architecture that had been pre-trained to categorize objects on the Imagenet database, which contains millions of photographs from hundreds of object categories \cite{deng2009imagenet}.

Despite the fact that drawings are highly abstracted away from natural visual inputs, a relatively modest amount of `fine-tuning' of models previously trained only on photos has been shown to lead to vast improvements in sketch categorization, and discerning image-level correspondences between sketches and photos \cite[]{sangkloy2016sketchy}. Even without fine-tuning, prior work has shown a striking isomorphism in the similarity structure \cite{kriegeskorte2008matching} of object categories in drawings and photos at higher layers in these models \cite[]{fan2015common}.

\cite{peterson2016adapting}

\subsubsection*{Social inference module}

To instantiate communicative flexibility in context, we generalize the Rational Speech Act (RSA) modeling framework, originally developed for linguistic communication. At the core of the RSA framework, as applied to linguistic communication, is the Gricean proposal that speakers aim to produce utterances that are concise yet informative. We adapt this proposal to visual communication by assuming that sketchers also aim to produce drawings that are both simple and informative. 

Formally, the sketcher, $\mathcal{S}$, is a decision-theoretic agent who produces drawings, $d$, by (soft) maximizing their utility function, $U$, given a particular object referent, $o$: 

\begin{equation}
\mathcal{S}(d|o) \propto e^{\alpha U(d,o)}
\end{equation}

Here, $\alpha$ is a softmax parameter. As $\alpha \rightarrow \infty$, the sketcher approaches the optimal maximizing policy. 

The sketcher's utility function, $U$, trades off the extent to which the drawing is informative to the viewer, $\mathcal{V}$, and the cost of producing the drawing, $C(d)$. This notion of informativity is defined by the (natural log) probability that a viewer would select the true object referent given the drawing and current context:

\begin{equation} \label{sketcher_utility}
U(d, o) = \ln \mathcal{V}(o|d) - C(d)
\end{equation}

This sketcher $\mathcal{S}$ is \emph{pragmatic} because they evaluate informativity by considering rational viewer agent ($\mathcal{V}$) who assigns a score to the correspondence between the drawing and each object that is proportional to the perceptual similarity, $P$, between the drawing and each potential object referent in context:

\begin{equation} \label{literal_viewer_score}
\mathcal{V}(o|d) \propto P_{o,d}
\end{equation}

To compute these perceptual similarity scores, the sketcher agent (who can also view the sketch and objects) extracts a feature vector representation from each of the target and distractor objects using the fixed visual encoder, and computes the distances between each of these vectors and the potential sketches it could produce. 

This has the effect of biasing the model to depict properties of the target object that distinguish it from the distractors, while also preferring sketches that are less costly to produce, $C(d)$, i.e., requiring fewer strokes or less ink.

\section*{Results}

This experiment tests a key prediction of the pragmatic visual communicator hypothesis. If sketchers aim to make their sketches informative without being too costly to produce, they will modulate the level of detail in their drawings according to the set of alternative hypotheses under consideration by the viewer. More specifically, we predict that the degree of visual similarity between alternatives will influence the \emph{accuracy} and the \emph{efficiency} with which participants convey target concepts. 

In the \textit{close} condition, the sketcher should make detailed drawings to highlight within-category distinctions, even though this requires additional strokes; however, in the \textit{far} condition where the alternatives are highly dissimilar from the target, the sketcher can provide simpler drawings by omitting excess detail.

Drawing \emph{accuracy} will be measured as the proportion of trials on which the sketcher's communication partner correctly identified the target in context. Drawing \emph{efficiency} will be quantified in two ways: (1) the number of strokes required to reach the minimum target rank (i.e., maximum classifier evidence) during that trial; and (2) the time (in seconds) taken to complete each drawing. The intuition for the stroke-based measure of efficiency is that it reflects the cost associated with producing more detailed sketches, such that a sketch with fewer strokes that reliably evokes the target in context is preferred to a more detailed sketch, all else equal. Our hypothesis makes three predictions concerning these measures of accuracy and efficiency. If sketchers are sensitive to the range of alternatives under consideration by the viewer we expect to observe: (1) \emph{higher identification accuracy} and (2) \emph{higher efficiency} in the \textit{far} condition than in the \textit{close} condition. 

% We operationalize the notion of `simplicity' through a small set of derived stroke-level measures. First, we expect the raw number of strokes per trial to be higher in the close condition than the far condition. Second, we expect the the length of the svg string to be higher in the close condition than the far condition, which should be correlated with raw number of strokes but accounts for the complexity of each stroke. Finally, we will conduct a range of model-based analyses using features extracted from modern convolutional neural networks to rigorously evaluate pragmatic and non-pragmatic accounts of communication under different assumptions about how the visual system computes similarity. 

\subsection*{Task performance}

\subsection*{Cost of sketching}

\subsection*{Informativity of sketches}

\subsection*{Sketcher model comparison}


\subsubsection*{Code availability} The code for the analyses presented in this article is publicly available in a Github repository at: XX.

\subsubsection*{Data availability} The data presented in this article are publicly available at this URL: XX.

% \acknowledgments
% This work was supported by XX. Thanks to XX, XX, and XX for helpful comments. 

% \authorcontributions 
% 

% \nocite{*}
\bibliography{references}
\bibliographystyle{apalike}
\end{document}
